{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanp23/scsd-ddm-class/blob/main/gmm_anomaly_detection_app_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "from flask import Flask, request, jsonify\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- Constants ---\n",
        "MODEL_DIR = \"model_artifacts\"\n",
        "MODEL_FILE = os.path.join(MODEL_DIR, \"gmm_model.pkl\")\n",
        "SCALER_FILE = os.path.join(MODEL_DIR, \"scaler.pkl\")\n",
        "THRESHOLD_FILE = os.path.join(MODEL_DIR, \"threshold.pkl\")\n",
        "ANOMALY_THRESHOLD_PERCENTILE = 99.0  # We'll flag anything \"weirder\" than 99% of normal data\n",
        "\n",
        "# --- 1. Dataset Generation ---\n",
        "\n",
        "def generate_data(n_normal=1000, n_anomalies=50):\n",
        "    \"\"\"\n",
        "    Generates a synthetic 2D dataset with two normal clusters and sparse anomalies.\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): Feature data (n_samples, 2)\n",
        "        y (np.ndarray): Labels (0 for normal, 1 for anomaly)\n",
        "    \"\"\"\n",
        "    print(\"Generating synthetic data...\")\n",
        "    # Cluster 1 (Normal)\n",
        "    cluster1 = np.random.multivariate_normal(\n",
        "        mean=[2, 3],\n",
        "        cov=[[1, 0.5], [0.5, 1]],\n",
        "        size=n_normal // 2\n",
        "    )\n",
        "    # Cluster 2 (Normal)\n",
        "    cluster2 = np.random.multivariate_normal(\n",
        "        mean=[-2, -3],\n",
        "        cov=[[1, -0.3], [-0.3, 0.8]],\n",
        "        size=n_normal // 2\n",
        "    )\n",
        "\n",
        "    # Combine normal data\n",
        "    X_normal = np.vstack([cluster1, cluster2])\n",
        "    y_normal = np.zeros(n_normal)\n",
        "\n",
        "    # Anomalies (sparse, from a wide uniform distribution)\n",
        "    X_anomalies = np.random.uniform(low=-15, high=15, size=(n_anomalies, 2))\n",
        "    y_anomalies = np.ones(n_anomalies)\n",
        "\n",
        "    # Combine all data\n",
        "    X = np.vstack([X_normal, X_anomalies])\n",
        "    y = np.concatenate([y_normal, y_anomalies])\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    indices = np.arange(len(y))\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# --- 2. Model Training ---\n",
        "\n",
        "def train_and_save_model(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains the GMM anomaly detection model and saves artifacts.\n",
        "\n",
        "    We follow a semi-supervised approach:\n",
        "    1. Scale ALL data.\n",
        "    2. Train the GMM *only* on known \"normal\" data (y_train == 0).\n",
        "    3. Determine the anomaly threshold based on the scores of this normal data.\n",
        "    4. Save the scaler, model, and threshold.\n",
        "    \"\"\"\n",
        "    print(\"Starting model training...\")\n",
        "\n",
        "    # Ensure artifact directory exists\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    # 1. Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    # 2. Isolate normal data for training\n",
        "    # This is a key step for this type of anomaly detection\n",
        "    X_normal_scaled = X_train_scaled[y_train == 0]\n",
        "\n",
        "    if len(X_normal_scaled) == 0:\n",
        "        raise ValueError(\"No 'normal' data (y_train == 0) found in the training set.\")\n",
        "\n",
        "    print(f\"Training GMM on {len(X_normal_scaled)} 'normal' samples...\")\n",
        "\n",
        "    # We choose 2 components, matching our data generation\n",
        "    gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
        "    gmm.fit(X_normal_scaled)\n",
        "\n",
        "    # 3. Determine the anomaly threshold\n",
        "    # The anomaly score is the *negative* log-likelihood.\n",
        "    # Higher score = more anomalous.\n",
        "    normal_scores = -gmm.score_samples(X_normal_scaled)\n",
        "\n",
        "    # Set the threshold at a high percentile of the \"normal\" scores\n",
        "    threshold = np.percentile(normal_scores, ANOMALY_THRESHOLD_PERCENTILE)\n",
        "\n",
        "    print(f\"Anomaly threshold (99th percentile of normal data) set to: {threshold:.4f}\")\n",
        "\n",
        "    # 4. Save artifacts\n",
        "    joblib.dump(gmm, MODEL_FILE)\n",
        "    print(f\"Saved model to {MODEL_FILE}\")\n",
        "\n",
        "    joblib.dump(scaler, SCALER_FILE)\n",
        "    print(f\"Saved scaler to {SCALER_FILE}\")\n",
        "\n",
        "    joblib.dump(threshold, THRESHOLD_FILE)\n",
        "    print(f\"Saved threshold to {THRESHOLD_FILE}\")\n",
        "\n",
        "    return gmm, scaler, threshold\n",
        "\n",
        "# --- 3. Model Evaluation ---\n",
        "\n",
        "def evaluate_model(gmm, scaler, threshold, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the held-out test set.\n",
        "    \"\"\"\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    # Scale test data\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Calculate anomaly scores\n",
        "    test_scores = -gmm.score_samples(X_test_scaled)\n",
        "\n",
        "    # Classify as anomaly (1) or normal (0)\n",
        "    y_pred = (test_scores > threshold).astype(int)\n",
        "\n",
        "    print(\"\\n--- Model Evaluation Report ---\")\n",
        "    print(f\"Test samples: {len(y_test)}\")\n",
        "    print(f\"Predicted anomalies (score > {threshold:.4f}): {np.sum(y_pred)}\")\n",
        "    print(f\"Actual anomalies in test set: {np.sum(y_test)}\")\n",
        "\n",
        "    # Print classification report\n",
        "    # Note: '1' is the 'anomaly' class\n",
        "    print(classification_report(y_test, y_pred, target_names=['Normal (0)', 'Anomaly (1)']))\n",
        "    print(\"---------------------------------\")\n",
        "\n",
        "\n",
        "# --- 4. Flask API Serving ---\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load all model artifacts into memory on startup\n",
        "try:\n",
        "    print(\"Loading model artifacts for API...\")\n",
        "    gmm_model = joblib.load(MODEL_FILE)\n",
        "    scaler_model = joblib.load(SCALER_FILE)\n",
        "    threshold_value = joblib.load(THRESHOLD_FILE)\n",
        "    print(\"Model artifacts loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n*** WARNING: Model artifacts not found! ***\")\n",
        "    print(\"Please run this script directly (python gmm_anomaly_detection_app.py)\")\n",
        "    print(\"to train the model and create the files before starting the API.\")\n",
        "    gmm_model = None\n",
        "    scaler_model = None\n",
        "    threshold_value = None\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return jsonify({\"message\": \"GMM Anomaly Detection API is running!\"})\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict_anomaly():\n",
        "    \"\"\"\n",
        "    Main prediction endpoint.\n",
        "    Expects JSON: {\"features\": [x, y]}\n",
        "    \"\"\"\n",
        "    if not gmm_model:\n",
        "        return jsonify({\"error\": \"Model is not loaded. Train the model by running the script.\"}), 500\n",
        "\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "\n",
        "        if 'features' not in data:\n",
        "            return jsonify({\"error\": \"Missing 'features' key in JSON payload.\"}), 400\n",
        "\n",
        "        features = data['features']\n",
        "\n",
        "        if not isinstance(features, list) or len(features) != 2:\n",
        "            return jsonify({\"error\": \"Features must be a list of 2 numbers, e.g., [1.2, 3.4]\"}), 400\n",
        "\n",
        "        # 1. Convert to numpy array\n",
        "        point = np.array(features).reshape(1, -1)\n",
        "\n",
        "        # 2. Scale using the *loaded* scaler\n",
        "        point_scaled = scaler_model.transform(point)\n",
        "\n",
        "        # 3. Calculate anomaly score\n",
        "        score = -gmm_model.score_samples(point_scaled)[0]\n",
        "\n",
        "        # 4. Compare with threshold\n",
        "        is_anomaly = bool(score > threshold_value)\n",
        "\n",
        "        # 5. Return result\n",
        "        return jsonify({\n",
        "            \"is_anomaly\": is_anomaly,\n",
        "            \"anomaly_score\": score,\n",
        "            \"threshold\": threshold_value,\n",
        "            \"input_features\": features\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"An unexpected error occurred: {str(e)}\"}), 500\n",
        "\n",
        "# --- 5. Main Execution (Train then Serve) ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Part 1: Train and Evaluate ---\n",
        "\n",
        "    # 1. Generate Data\n",
        "    X, y = generate_data()\n",
        "\n",
        "    # 2. Split data (stratify to keep anomaly ratio consistent)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    # 3. Train and Save Model\n",
        "    # We pass y_train so the function can isolate the \"normal\" data\n",
        "    gmm, scaler, threshold = train_and_save_model(X_train, y_train)\n",
        "\n",
        "    # 4. Evaluate Model\n",
        "    evaluate_model(gmm, scaler, threshold, X_test, y_test)\n",
        "\n",
        "    # --- Part 2: Start the API ---\n",
        "    print(\"\\n--- Starting Flask API Server ---\")\n",
        "    print(\"The server is now running.\")\n",
        "    print(\"You can send POST requests to http://127.0.0.1:5000/predict\")\n",
        "    print(\"\\nExample (Normal Point):\")\n",
        "    print(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d '{\\\"features\\\": [2, 2]}' http://127.0.0.1:5000/predict\")\n",
        "    print(\"\\nExample (Anomaly Point):\")\n",
        "    print(\"curl -X POST -H \\\"Content-Type: application/json\\\" -d '{\\\"features\\\": [10, 10]}' http://127.0.0.1:5000/predict\")\n",
        "\n",
        "    # Reload the models in the global scope for the app\n",
        "    # (This ensures the app context has the newly trained models)\n",
        "    gmm_model = joblib.load(MODEL_FILE)\n",
        "    scaler_model = joblib.load(SCALER_FILE)\n",
        "    threshold_value = joblib.load(THRESHOLD_FILE)\n",
        "    print(\"\\nGlobal API models reloaded with new training data.\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    # `debug=False` is safer for a \"production\" example,\n",
        "    # but `debug=True` is fine for development.\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model artifacts for API...\n",
            "\n",
            "*** WARNING: Model artifacts not found! ***\n",
            "Please run this script directly (python gmm_anomaly_detection_app.py)\n",
            "to train the model and create the files before starting the API.\n",
            "Generating synthetic data...\n",
            "Starting model training...\n",
            "Training GMM on 800 'normal' samples...\n",
            "Anomaly threshold (99th percentile of normal data) set to: 4.4839\n",
            "Saved model to model_artifacts/gmm_model.pkl\n",
            "Saved scaler to model_artifacts/scaler.pkl\n",
            "Saved threshold to model_artifacts/threshold.pkl\n",
            "Evaluating model on test set...\n",
            "\n",
            "--- Model Evaluation Report ---\n",
            "Test samples: 210\n",
            "Predicted anomalies (score > 4.4839): 11\n",
            "Actual anomalies in test set: 10.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       1.00      0.99      1.00       200\n",
            " Anomaly (1)       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           1.00       210\n",
            "   macro avg       0.95      1.00      0.97       210\n",
            "weighted avg       1.00      1.00      1.00       210\n",
            "\n",
            "---------------------------------\n",
            "\n",
            "--- Starting Flask API Server ---\n",
            "The server is now running.\n",
            "You can send POST requests to http://127.0.0.1:5000/predict\n",
            "\n",
            "Example (Normal Point):\n",
            "curl -X POST -H \"Content-Type: application/json\" -d '{\"features\": [2, 2]}' http://127.0.0.1:5000/predict\n",
            "\n",
            "Example (Anomaly Point):\n",
            "curl -X POST -H \"Content-Type: application/json\" -d '{\"features\": [10, 10]}' http://127.0.0.1:5000/predict\n",
            "\n",
            "Global API models reloaded with new training data.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og8FKrX6Uai2",
        "outputId": "37307254-c144-4d98-f218-56a1d4365fb8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}